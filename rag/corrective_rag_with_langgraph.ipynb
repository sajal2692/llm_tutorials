{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VjV1PGk7Bs7s"
      },
      "outputs": [],
      "source": [
        "# !pip install langgraph langchain langchain_openai chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# imports\n",
        "import os\n",
        "\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "UwN1No7mFied"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = # enter your openai api key here"
      ],
      "metadata": {
        "id": "6Fp6rP5cqcqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingest the data, a markdown file with information on work experience, etc.\n",
        "\n",
        "Process the markdown document, which includes details such as work experience, by segmenting it at markdown header points to create each chunk. This ensures that each segment maintains its integrity, encapsulating the relevant data within."
      ],
      "metadata": {
        "id": "nGEuMyMsAtip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ingesting data\n",
        "markdown_path = \"source.md\"\n",
        "# read the markdown file and return the full document as a string\n",
        "with open(markdown_path, \"r\") as file:\n",
        "    full_markdown_document = file.read()\n",
        "\n",
        "# split the data into chunks based on the markdown heading\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "chunked_documents = markdown_splitter.split_text(full_markdown_document)\n",
        "\n",
        "# create a vector store\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(chunked_documents, embeddings_model)\n",
        "\n",
        "# create retriever\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "np3IRXO6Fig1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)"
      ],
      "metadata": {
        "id": "lyh5FH7dr6fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the rag chain"
      ],
      "metadata": {
        "id": "U-Yojp-QBRYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_prompt = \"\"\"You are an AI  assistant. Your main task is to answer questions people may have about Sajal.\n",
        "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "uvYG2kyUFijH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of good results"
      ],
      "metadata": {
        "id": "qs3aD9q6BUTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"When did Sajal graduate from University of Melbourne?\")"
      ],
      "metadata": {
        "id": "ssSjxzizFilN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3d468071-9df6-47ca-ef2c-224f9705c1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sajal graduated from the University of Melbourne with a Master of Information Technology, majoring in Computing, in August 2016.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"What did Sajal do at Unscrambl?\")"
      ],
      "metadata": {
        "id": "8Ptt39MyFinj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "2b9eae14-3d71-460e-dfef-efac5d5570b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'At Unscrambl, Sajal was a key member of the NLP Engineering team, where he helped enhance the natural language understanding of their business analytics platform, focusing on advancing Named Entity Recognition (NER), intent recognition, and ANNOY model functionalities. He developed the Natural Language to SQL system data preparation pipeline using NLTK and spaCy, significantly reducing manual effort and boosting system efficiency. Additionally, Sajal collaborated in designing and developing NLP-driven chatbot products and led the deployment of these solutions for clients across Asia, impacting over 100,000 monthly users.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of subpar results"
      ],
      "metadata": {
        "id": "biAbW43zBW30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# incorrect result\n",
        "rag_chain.invoke(\"How many countries has sajal worked in?\")"
      ],
      "metadata": {
        "id": "pUu1VudfFip6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ecf28cb6-4dcd-4959-cb70-ee8dc8e0c01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The provided documents do not specify the exact number of countries Sajal has worked in. However, his education and mentoring activities suggest he has connections to Australia and India, and possibly interacts with international students globally through his role as a mentor at Udacity. Without more specific information on his professional work locations, it's not possible to give a precise count of countries he has worked in.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check what documents were retrieved from the vector db\n",
        "retriever.get_relevant_documents(\"How many countries has sajal worked in?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib21OztLy8iv",
        "outputId": "fa4ae7a8-7c61-4124-ae5d-97de253a85af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='# Sajal Sharma  \\n## Contact Info  \\n+65 9077-9093 | contact@sajalsharma.com | [LinkedIn](linkedin.com/in/sajals) | [GitHub](github.com/sajal2692)', metadata={'Header 1': 'Sajal Sharma', 'Header 2': 'Contact Info'}),\n",
              " Document(page_content='## Languages  \\n- Hindi (Native or Bilingual)\\n- English (Native or Bilingual)\\n- German (Elementary)', metadata={'Header 1': 'Sajal Sharma', 'Header 2': 'Languages'}),\n",
              " Document(page_content='## Activities  \\n- Mentor & Project Reviewer, Udacity: Coached 100+ international students enrolled in Data Science courses. Recognised as an elite mentor in 2021 with A+ mentor performance grade based on student feedback scores.\\n- Mentor, STEM Industry Mentoring Programme, The University of Melbourne: Jul 2020 - Present\\n- Creator, Data Science Portfolio: Github repo with 900+ stars showcasing various classical Data Science projects.', metadata={'Header 1': 'Sajal Sharma', 'Header 2': 'Activities'}),\n",
              " Document(page_content='## Education  \\n**The University of Melbourne**\\nMaster of Information Technology, Major in Computing\\nMelbourne, Australia\\nAug 2014 – Aug 2016  \\n**Bharatiya Vidyapeeth University**\\nBachelor of Computer Applications\\nNew Delhi, India\\nJul 2010 – Jul 2013', metadata={'Header 1': 'Sajal Sharma', 'Header 2': 'Education'})]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are non chunks that can directly answer the given question, the similarity search finds it to find relevant information.\n",
        "\n",
        "Another example of a similar case:"
      ],
      "metadata": {
        "id": "ohXWZID9BiHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# incorrect / incomplete result\n",
        "rag_chain.invoke(\"list all the positions that sajal has held throughout his career\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "YO4yTlYixtYj",
        "outputId": "5ab48db3-a234-4e7c-e525-90fa54219541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Throughout his career, Sajal has held the following positions:\\n1. Mentor & Project Reviewer at Udacity\\n2. Mentor at the STEM Industry Mentoring Programme, The University of Melbourne\\n3. Creator of a Data Science Portfolio on GitHub\\n4. Senior AI Engineer at Splore, a Temasek-backed AI startup (contracted via Unscrambl), Singapore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a Corrective RAG workflow using LangGraph\n",
        "\n",
        "1. Grade retrieved documents based on the question\n",
        "2. If no relevant documents found, then pass in the whole source document as the context."
      ],
      "metadata": {
        "id": "b6FrLnvYyPx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the state class which holds data related to the current state\n",
        "from typing import Dict, TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        keys: A dictionary where each key is a string.\n",
        "    \"\"\"\n",
        "    keys: Dict[str, any]"
      ],
      "metadata": {
        "id": "COSLhZ-CyXch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the nodes of the graph"
      ],
      "metadata": {
        "id": "sppn3fce6yXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents(state):\n",
        "  \"\"\"Node to retrieve documents, by using the query from the state\"\"\"\n",
        "  print(\"---RETRIEVE DOCUMENTS---\") # print statements to track flow\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = retriever.get_relevant_documents(question)\n",
        "  return {\"keys\": {\"question\": question, \"documents\": documents}}"
      ],
      "metadata": {
        "id": "8danfQWH5H00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_answer_chain = rag_prompt_template | llm | StrOutputParser()\n",
        "def generate_with_retrieved_documents(state):\n",
        "  \"\"\"Node to generate answer using retrieved documents\"\"\"\n",
        "  print(\"---GENERATE USING RETRIEVED DOCUMENTS---\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "  answer = generation_answer_chain.invoke({\"question\": question, \"context\": documents})\n",
        "  return {\"keys\": {\"question\": question, \"response\": answer}}"
      ],
      "metadata": {
        "id": "7qUGr-yNEKil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "grader_prompt = \"\"\"\n",
        "You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "Retrieved document: \\n\\n {context} \\n\\n\n",
        "User Question: {question} \\n\n",
        "When assessing the relevance of a retrieved document to a user question, consider whether the document can provide a complete answer to the question posed. A document is considered relevant only if it contains all the necessary information to fully answer the user's inquiry without requiring additional context or assumptions.\n",
        "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
        "Do not return anything other than a 'yes' or 'no'.\n",
        "\"\"\"\n",
        "\n",
        "grader_prompt_template = PromptTemplate(template=grader_prompt, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# pydantic class for grade, to be used with openai function calling\n",
        "class grade(BaseModel):\n",
        "    \"\"\"Binary score for relevance check.\"\"\"\n",
        "    binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
        "\n",
        "grade_tool_openai = convert_to_openai_tool(grade)\n",
        "\n",
        "llm_with_grader_tool = llm.bind(\n",
        "    tools=[grade_tool_openai],\n",
        "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}}\n",
        ")\n",
        "\n",
        "tool_parser = PydanticToolsParser(tools=[grade])\n",
        "\n",
        "grader_chain = grader_prompt_template | llm_with_grader_tool | tool_parser\n",
        "\n",
        "def grade_documents(state):\n",
        "  \"\"\"Node to grade documents, filter out irrelevant documents and assess whether need to run generation on whole document\"\"\"\n",
        "  print(\"---GRADE DOCUMENTS---\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  filtered_documents = []\n",
        "  run_with_all_data = False\n",
        "  for doc in documents:\n",
        "    score = grader_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    grade = score[0].binary_score\n",
        "    if grade == \"yes\":\n",
        "      print(\"---GRADE: FOUND RELEVANT DOCUMENT---\")\n",
        "      filtered_documents.append(doc)\n",
        "  if not filtered_documents:\n",
        "    print(\"---GRADE: DID NOT FIND ANY RELEVANT DOCUMENTS\")\n",
        "    run_with_all_data = True\n",
        "\n",
        "  return {\n",
        "      \"keys\": {\n",
        "          \"documents\": filtered_documents,\n",
        "          \"question\": question,\n",
        "          \"run_with_all_data\": run_with_all_data\n",
        "          }\n",
        "      }"
      ],
      "metadata": {
        "id": "dRCk5YuH6u4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_using_all_data(state):\n",
        "  \"\"\"Node to generate the answer using the complete document\"\"\"\n",
        "  print(\"---GENERATING ANSWER USING ALL DATA\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  answer = generation_answer_chain.invoke({\"question\": question, \"context\": full_markdown_document})\n",
        "  return {\"keys\": {\"question\": question, \"response\": answer}}"
      ],
      "metadata": {
        "id": "4JqhROje5cD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the conditional edge"
      ],
      "metadata": {
        "id": "MWPLoAMwHYnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_to_use_all_data(state):\n",
        "  \"\"\"Conditional edge that decides the next node to run\"\"\"\n",
        "  state_dict = state[\"keys\"]\n",
        "  run_with_all_data = state_dict[\"run_with_all_data\"]\n",
        "\n",
        "  if run_with_all_data:\n",
        "      return \"generate_answer_using_all_data\"\n",
        "  else:\n",
        "      return \"rag\""
      ],
      "metadata": {
        "id": "7mwt2RC6FiuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the graph"
      ],
      "metadata": {
        "id": "M95D_qfoB5y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        keys: A dictionary where each key is a string.\n",
        "    \"\"\"\n",
        "    keys: Dict[str, any]\n",
        "\n",
        "\n",
        "def compile_graph():\n",
        "  workflow = StateGraph(GraphState)\n",
        "  ### define the nodes\n",
        "  workflow.add_node(\"retrieve\", retrieve_documents)\n",
        "  workflow.add_node(\"grade_documents\", grade_documents)\n",
        "  workflow.add_node(\"generate_answer_with_retrieved_documents\", generate_with_retrieved_documents)\n",
        "  workflow.add_node(\"generate_answer_using_all_data\", generate_answer_using_all_data)\n",
        "  ### build the graph\n",
        "  workflow.set_entry_point(\"retrieve\")\n",
        "  workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "  workflow.add_conditional_edges(\n",
        "      \"grade_documents\",\n",
        "      decide_to_use_all_data,\n",
        "      {\n",
        "          \"rag\": \"generate_answer_with_retrieved_documents\",\n",
        "          \"generate_answer_using_all_data\": \"generate_answer_using_all_data\",\n",
        "      }\n",
        "  )\n",
        "  workflow.add_edge(\"generate_answer_with_retrieved_documents\", END)\n",
        "  workflow.add_edge(\"generate_answer_using_all_data\", END)\n",
        "  ### compile the graph\n",
        "  app = workflow.compile()\n",
        "  return app"
      ],
      "metadata": {
        "id": "69_1m-wiHjmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the graph"
      ],
      "metadata": {
        "id": "EfVoPoi1B8_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = compile_graph()\n",
        "def response_from_graph(question):\n",
        "  \"\"\"Returns the response from the graph\"\"\"\n",
        "  return app.invoke({\"keys\": {\"question\": question}})[\"keys\"][\"response\"]"
      ],
      "metadata": {
        "id": "C1eLbw4MIiBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying on the same example as previously"
      ],
      "metadata": {
        "id": "8SjZr2vgCArd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing out the flow with crag\n",
        "print(response_from_graph(\"How many countries has sajal worked in?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhLKqhzDINZZ",
        "outputId": "ad78af7b-5260-45c6-882d-3c9d6ffeacc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE DOCUMENTS---\n",
            "---GRADE DOCUMENTS---\n",
            "---GRADE: DID NOT FIND ANY RELEVANT DOCUMENTS\n",
            "---GENERATING ANSWER USING ALL DATA\n",
            "Sajal has worked in at least three countries: Singapore, the Philippines, and India. His work in Singapore is mentioned with OneByZero and Splore, a Temasek-backed AI startup. Additionally, he developed a proof of concept for a major bank in the Philippines and was a key member of Unscrambl's NLP Engineering team in India.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_from_graph(\"list all the positions that sajal has held throughout his career\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTGyZFlXIgrZ",
        "outputId": "7ab20cdc-eadb-48e2-9ff0-ad362a907935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE DOCUMENTS---\n",
            "---GRADE DOCUMENTS---\n",
            "---GRADE: DID NOT FIND ANY RELEVANT DOCUMENTS\n",
            "---GENERATING ANSWER USING ALL DATA\n",
            "Throughout his career, Sajal has held the following positions:\n",
            "1. Lead AI Engineer at OneByZero (contracted via Unscrambl), Singapore.\n",
            "2. Senior AI Engineer at Splore, a Temasek-backed AI startup (contracted via Unscrambl), Singapore.\n",
            "3. Senior Machine Learning Engineer at Unscrambl, India.\n",
            "4. Machine Learning Engineer at Unscrambl, India.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph is able to handle cases where the retrieved chunks can be used to answer the question."
      ],
      "metadata": {
        "id": "ePiE1m9hCHI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_from_graph(\"Has sajal created any popular github repositories?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pu-dyXrJ6LQ",
        "outputId": "d38ead0f-cbdf-49f7-fae1-f9ad1172d69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE DOCUMENTS---\n",
            "---GRADE DOCUMENTS---\n",
            "---GRADE: FOUND RELEVANT DOCUMENT---\n",
            "---GRADE: FOUND RELEVANT DOCUMENT---\n",
            "---GRADE: FOUND RELEVANT DOCUMENT---\n",
            "---GRADE: FOUND RELEVANT DOCUMENT---\n",
            "---GENERATE USING RETRIEVED DOCUMENTS---\n",
            "Yes, Sajal has created a popular GitHub repository. His Data Science Portfolio on GitHub has garnered over 900 stars, showcasing various classical Data Science projects. This indicates a significant level of recognition and appreciation from the GitHub community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YNmCyviwKPTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}